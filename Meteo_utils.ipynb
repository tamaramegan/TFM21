{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteo Station Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_meteo_zip(path, folder,only_csv =True):\n",
    "    ### access files \n",
    "    ziplist = glob.glob(f'{path}/*.zip')\n",
    "    \n",
    "    ### access folder\n",
    "    zf = zipfile.ZipFile(f'{ziplist[folder]}')\n",
    "    \n",
    "    #extract dates of each of the zipfiles from file name\n",
    "    a = []\n",
    "    for i in range(len(zf.namelist())):\n",
    "        date_str = zf.namelist()[i].split('/')[1]\n",
    "        try:\n",
    "            date_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        except:\n",
    "            continue\n",
    "        a.append(date_date)\n",
    "        \n",
    "    # get unique values\n",
    "    unique_dates = list(set(a))\n",
    "    # order by date\n",
    "    unique_dates.sort()\n",
    "    #convert back to string\n",
    "    str_dates = []\n",
    "    for l in range(len(unique_dates)):\n",
    "        str_dates.append(unique_dates[l].strftime('%Y-%m-%d'))\n",
    "        \n",
    "    ## Reorder files\n",
    "    files_by_date = []\n",
    "    for dates in str_dates:\n",
    "        if only_csv:\n",
    "            r = re.compile(f'.*/{dates}/.*.csv') # only csv files\n",
    "        else:\n",
    "            r = re.compile(f'.*/{dates}/.*') # all files (including excel sheets)\n",
    "        newlist = list(filter(r.match, zf.namelist())) \n",
    "        files_by_date.extend(newlist)\n",
    "    \n",
    "    if len(files_by_date)==0:\n",
    "        r = re.compile(f'.*.csv') # only csv files\n",
    "        files_by_date = list(filter(r.match, zf.namelist())) \n",
    "    \n",
    "    return files_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meteo_csv(path, folder, file): \n",
    "    ### access files \n",
    "    ziplist = glob.glob(f'{path}/*.zip')\n",
    "    \n",
    "    ### access folder\n",
    "    zf = zipfile.ZipFile(f'{ziplist[folder]}')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(zf.open(file), sep = ';', encoding = 'latin-1')\n",
    "        ## get unmmaned columns:\n",
    "        r = re.compile('Unnamed:') # get unnamed columns\n",
    "        unnamed = list(filter(r.match, df.columns))\n",
    "        for col in unnamed:\n",
    "            df.drop(columns = [col],inplace =True)\n",
    "            \n",
    "        if len(df.columns) == 1:\n",
    "            df = df[df.columns[0]].str.split(',',expand=True)\n",
    "            if len(df.columns) == 1:\n",
    "                df = df[df.columns[0]].str.split('.',expand=True)\n",
    "                df[3] =df[3].astype(str).str.cat(df[4].astype(str),sep=\".\")\n",
    "                df.drop(columns=4, inplace=True)\n",
    "            df = df.iloc[1:]\n",
    "        \n",
    "        if 'Station Name' in df:\n",
    "            df = pd.read_csv(zf.open(file), sep = ';', encoding = 'latin-1',skiprows=1)\n",
    "            for col in cols:\n",
    "                df.drop(columns = [col],inplace =True)\n",
    "            df.columns = ['Date_Time','Variable', 'Value']\n",
    "        print('Success sep \";\", latin encoding')\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(zf.open(file),header=None, encoding = 'latin-1')\n",
    "            print('Success regular csv, no header, latin encoding')\n",
    "        except:\n",
    "            print('Failed')\n",
    "            pass\n",
    "    \n",
    "    ## Detect and elminate empty columns- incluiding columns with low number of values (<1% of file)\n",
    "    df = df.replace('', np.nan)\n",
    "    empty_cols = [col for col in df.columns if df[col].isnull().all() or round(df[col].isnull().value_counts()[0]/len(df[col]),2) < 0.01]\n",
    "    if len(list(set(empty_cols) & set(['Temperatura','Humedad','Precipitacion','Direccion','Velocidad']))) <1:\n",
    "        df.drop(empty_cols,\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "    if len(df.columns)==5:\n",
    "        if df.columns.isin(['Fecha','Temperatura','Humedad','Precipitacion','Direccion','Velocidad']).sum()==5:\n",
    "            df = df\n",
    "        else: df.columns = ['Date', 'Time','Variable', 'Value','Type']\n",
    "    if len(df.columns)==2:\n",
    "        df.columns = ['Fecha','PAvg']\n",
    "    \n",
    "    # add frequency and file name as a variable\n",
    "    freq = ['1min','1MIN','5min','5MIN','1hora', '1HORA','30seg']\n",
    "    try:\n",
    "        df['Freq']= re.findall('|'.join(freq),file)[0].lower()\n",
    "    except:\n",
    "        if len(df.columns)==2:\n",
    "            df['Freq']='1day'\n",
    "            \n",
    "    df['file_name'] = file\n",
    "    print(f'file ={file}, cols = {len(df.columns)}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get unique variable names\n",
    "def get_unique_variables(files_by_date, path, folder):\n",
    "    summary_df = pd.DataFrame(columns = ['station','filename','colnum','colnames','nrows'])\n",
    "    ### access files \n",
    "    ziplist = glob.glob(f'{path}/*.zip')\n",
    "    \n",
    "    ### access folder\n",
    "    zf = zipfile.ZipFile(f'{ziplist[folder]}')\n",
    "    \n",
    "    for i in range(0,len(files_by_date)):\n",
    "    \n",
    "        df = read_meteo_csv(path =path,folder = folder,file =files_by_date[i])\n",
    "        if len(df.columns)==7:\n",
    "            try:\n",
    "                colname = [list(df.Variable.unique())]\n",
    "            except:\n",
    "                colname= [list(df.columns)]\n",
    "        else:\n",
    "            colname= [list(df.columns)]\n",
    "        \n",
    "        fill = {'station':files_by_date[i].split('/')[0].replace(\" \",\"\").replace(\".\",\"\"),\n",
    "            'filename':files_by_date[i],\n",
    "            'colnum':len(df.columns),\n",
    "            'colnames':colname,\n",
    "            'nrows':len(df)}\n",
    "        summary_df = summary_df.append(fill, ignore_index=True)\n",
    "        \n",
    "    export_name = f'{files_by_date[i].split(\"/\")[0].replace(\" \",\"\").replace(\".\",\"\")}'\n",
    "    repeat = list(set(['summary_'+export_name +'.csv']) & set(os.listdir(path)))\n",
    "    if len(repeat)>0:\n",
    "        summary_df.to_csv(f'{path}/summary_{export_name}_{len(repeat)+1}.csv')\n",
    "    else :\n",
    "        summary_df.to_csv(f'{path}/summary_{export_name}.csv')\n",
    "    \n",
    "    all_var = []\n",
    "    for i in range(0,len(summary_df)):\n",
    "        file_vars= summary_df.colnames[i][0]\n",
    "        not_in_list =list(set(file_vars) - set(all_var))\n",
    "        all_var.extend(not_in_list) \n",
    "    all_var = sorted([str(i) for i in all_var])\n",
    "    \n",
    "    return all_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_df(df, replace_values):\n",
    "    \n",
    "    ## delete any complete empty rows\n",
    "    df.dropna(inplace=True, how = 'all')\n",
    "        \n",
    "    ## merge date + time\n",
    "    ## replace variable names\n",
    "    ## convert to long format with same column names and order\n",
    "   \n",
    "    if df.apply(lambda row: row.astype(str).str.contains('OFICINA').any(), axis=1).sum() == 0:\n",
    "    #if (('Fecha' in df) & (len(df[df['Fecha']=='OFICINA']) <0)):\n",
    "    \n",
    "        #1. Wide format datasets\n",
    "        r = re.compile('[Ff][Ee][Cc][Hh][Aa]') # get unnamed columns\n",
    "        fecha = list(filter(r.match, df.columns))\n",
    "        if len(fecha) > 0:\n",
    "            df['Date_Time'] = pd.to_datetime(df[fecha[0]], errors ='coerce')\n",
    "            df.drop(columns =[fecha[0]],inplace =True)\n",
    "            df.dropna(subset = ['Date_Time'])\n",
    "            \n",
    "            ### delete extra variables\n",
    "            extra = ['HUMEDAD RELATIVA DEL AIRE % INST','RADIACION SOLAR GLOBAL W/mÂ² SUM', 'RADIACION SOLAR GLOBAL W/mÂ² PROM',\n",
    "                     'RADIACION SOLAR GLOBAL W/mÂ² MAX','RADIACION SOLAR REFLEJADA W/mÂ² PROM','TEMPERATURA AIRE Â°C INST', ]\n",
    "            for col in extra:\n",
    "                if col in df:\n",
    "                    df.drop(columns = [col],inplace =True)\n",
    "\n",
    "        \n",
    "            for variable in list(replace_values.keys()):\n",
    "                try:\n",
    "                    df = df.rename(columns={list(set(df.columns) & set(replace_values[variable]))[0]: variable})\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "            df['Type'] = np.nan\n",
    "    \n",
    "        #2. Long format datasets\n",
    "        else:\n",
    "            if 'Date_Time' in df:\n",
    "                df['Type'] = np.nan\n",
    "            else: \n",
    "                df['Date_Time']= pd.to_datetime(df['Date'] + ' ' + df['Time'], errors ='coerce')\n",
    "                df.drop(columns =['Date','Time'],inplace =True)\n",
    "                df.dropna(subset = ['Date_Time'])\n",
    "        \n",
    "            for variable in list(replace_values.keys()):\n",
    "                df =df.replace(to_replace =replace_values[variable], value = variable)\n",
    "    \n",
    "            #Convert to long format and keep all variables\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            df = df.reset_index().drop(columns = 'index')\n",
    "            freq = df.Freq.unique()[0]\n",
    "            file_name = df.file_name.unique()[0]\n",
    "            try:\n",
    "                df2 =df.pivot(index='Date_Time', columns='Variable').reset_index()\n",
    "                df = df.pivot(index='Date_Time', columns='Variable', values='Value').reset_index()\n",
    "            except:\n",
    "                df2 =pd.pivot_table(df,index='Date_Time', columns='Variable',aggfunc='first').reset_index()\n",
    "                df =pd.pivot_table(df,index='Date_Time', columns='Variable',values = 'Value',aggfunc='first').reset_index()\n",
    "            df['Freq'] = freq\n",
    "            df['Type'] = df2.Type.iloc[:,0]\n",
    "            df['file_name'] = file_name\n",
    "    \n",
    "    \n",
    "        ## Add variables that are not in the df but are in the general list\n",
    "        not_in_df = list(set(replace_values.keys())-set(df.columns))\n",
    "        for i in not_in_df:\n",
    "            df[i]=np.nan\n",
    "\n",
    "\n",
    "        keep = list(replace_values.keys())+ ['Date_Time', 'Type', 'Freq','file_name']\n",
    "        empty_cols = [col for col in df.columns if df[col].isnull().all() or round(df[col].isnull().value_counts()[0]/len(df[col]),2) < 0.01]\n",
    "        if len(list(set(empty_cols) - set(keep))) >0:\n",
    "            df.drop(list(set(empty_cols) - set(keep)),\n",
    "                axis=1,\n",
    "                inplace=True)\n",
    "    \n",
    "        ## Order columns so all dfs have the same structure\n",
    "        var_order = ['Date_Time', 'Type', 'Freq','file_name']\n",
    "        var_order.extend(sorted(df.loc[:,list(set(df.columns) - set(['Date_Time', 'Type', 'Freq','file_name']))]))\n",
    "        df = df.reindex(var_order, axis=1)\n",
    "    \n",
    "        ## make sure only the accepted variables are kept\n",
    "        not_good = list(set(keep) - set(df.columns))\n",
    "        df.drop(not_good)\n",
    "    \n",
    "        ## convert any ',' decimals to '.'\n",
    "        try:\n",
    "            df.iloc[:,4:] = df.iloc[:,4:].astype(float)\n",
    "        except:\n",
    "            cols= np.where(df.dtypes[4:]=='object')[0]+4\n",
    "            df.iloc[:,cols] = df.iloc[:,cols].apply(lambda x: x.str.replace(',','.')) \n",
    "            df.iloc[:,cols] = df.iloc[:,cols].apply(pd.to_numeric, errors='coerce')\n",
    "    else:\n",
    "        df = pd.DataFrame(columns =list(replace_values.keys())+ ['Date_Time', 'Type', 'Freq','file_name'])\n",
    "        var_order = ['Date_Time', 'Type', 'Freq','file_name']\n",
    "        var_order.extend(sorted(df.loc[:,list(set(df.columns) - set(['Date_Time', 'Type', 'Freq','file_name']))]))\n",
    "        df = df.reindex(var_order, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
