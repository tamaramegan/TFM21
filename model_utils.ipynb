{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data windowing\n",
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split window\n",
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create sequence of arrays of all the dataset\n",
    "def dataset_stack(train_df, window):\n",
    "    stack = []\n",
    "    for i in range(0,len(train_df[0::window])):\n",
    "        start_array = window*i\n",
    "        end_array = window*i + window\n",
    "        if len(train_df[start_array:end_array]) < window:\n",
    "            continue\n",
    "        else:\n",
    "            stack.append(np.array(train_df[start_array:end_array]))\n",
    "    return stack\n",
    "#stack = dataset_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='ATAvg', max_subplots=5):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "        if self.label_columns:\n",
    "            label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "        else:\n",
    "            label_col_index = plot_col_index\n",
    "            \n",
    "        if label_col_index is None:\n",
    "            continue\n",
    "            \n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "            predictions = model(inputs)\n",
    "            if self.label_width == 1:\n",
    "                plt.scatter(self.label_indices, predictions[0, -1, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            else:\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "            \n",
    "        plt.xlabel('Time [steps]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=batch_size,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window,patience=5):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]\n",
    "\n",
    "class ResidualWrapper(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "\n",
    "  def call(self, inputs, *args, **kwargs):\n",
    "    delta = self.model(inputs, *args, **kwargs)\n",
    "\n",
    "    # The prediction for each timestep is the input\n",
    "    # from the previous time step plus the delta\n",
    "    # calculated by the model.\n",
    "    return inputs + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n",
    "\n",
    "class RepeatBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs\n",
    "\n",
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "FeedBack.warmup = warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training=None):\n",
    "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "  predictions = []\n",
    "  # Initialize the lstm state\n",
    "  prediction, state = self.warmup(inputs)\n",
    "\n",
    "  # Insert the first prediction\n",
    "  predictions.append(prediction)\n",
    "\n",
    "  # Run the rest of the prediction steps\n",
    "  for n in range(1, self.out_steps):\n",
    "    # Use the last prediction as input.\n",
    "    x = prediction\n",
    "    # Execute one lstm step.\n",
    "    x, state = self.lstm_cell(x, states=state,\n",
    "                              training=training)\n",
    "    # Convert the lstm output to a prediction.\n",
    "    prediction = self.dense(x)\n",
    "    # Add the prediction to the output\n",
    "    predictions.append(prediction)\n",
    "\n",
    "  # predictions.shape => (time, batch, features)\n",
    "  predictions = tf.stack(predictions)\n",
    "  # predictions.shape => (batch, time, features)\n",
    "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "  return predictions\n",
    "\n",
    "FeedBack.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(self, model = None, plot_col = ['ATAvg','RHAvg'], plot=False):\n",
    "    \n",
    "#     accuracy={}\n",
    "#     for col in range(len(plot_col)):\n",
    "#         plot_col_index = self.column_indices[plot_col[col]]\n",
    "#         all_preds=[]\n",
    "#         all_labels =[]\n",
    "    \n",
    "#         n_batches = len(tuple(self.test))\n",
    "#         if self.shift==1:\n",
    "#             for i in range(n_batches):\n",
    "#                 for inputs, labels in self.test.take(i):  # iterate over batches\n",
    "\n",
    "#                     numpy_labels = labels.numpy() ### get labels\n",
    "#                     numpy_inputs = inputs.numpy() ### get inputs\n",
    "#                     preds = model(numpy_inputs) ### make prediction from trined model\n",
    "#                     numpy_preds = preds.numpy() ### get predictions\n",
    "\n",
    "#                     batch_pred =numpy_preds[:,-1,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "#                     batch_label = numpy_labels[:,-1,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "\n",
    "#                     all_preds.extend(batch_pred)\n",
    "#                     all_labels.extend(batch_label)\n",
    "\n",
    "\n",
    "#             r2=round(r2_score(all_labels, all_preds),3)\n",
    "#             mae = round(mean_absolute_error(all_labels, all_preds),3)\n",
    "\n",
    "#             accuracy[plot_col[col]] = {'r2':r2,\n",
    "#                                        'mae':mae}\n",
    "\n",
    "#             if plot:\n",
    "#                 ## One sactter plot per variable\n",
    "#                 slope, intercept, r_value, p_value, std_err = stats.linregress(all_labels, all_preds)\n",
    "#                 fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "#                 ax.scatter(all_labels, all_preds,edgecolors='k', c='#ff5555', s=32)\n",
    "#                 ax.set_xlabel(f'True Values {plot_col[col]} [de-normed]')\n",
    "#                 ax.set_ylabel(f'Predicted Values {plot_col[col]} [de-normed]')\n",
    "#                 lims = [math.floor(min(all_labels)-min(all_labels)*0.1), math.ceil(max(all_labels)+max(all_labels)*0.1)]\n",
    "#                 ax.set_xlim(lims)\n",
    "#                 ax.set_ylim(lims)\n",
    "#                 line = slope*np.array(all_labels)+intercept\n",
    "#                 ax.plot(all_labels, line, 'gray',label = f'r2 = {round(r_value**2,3)}')\n",
    "#                 ax.legend(loc = 'lower right')\n",
    "\n",
    "\n",
    "#         else:\n",
    "#             for i in range(n_batches):\n",
    "#                 #print(f'i = {i}')\n",
    "#                 for inputs, labels in self.test.take(i):  # iterate over batches\n",
    "\n",
    "#                     numpy_labels = labels.numpy() ### get labels\n",
    "#                     numpy_inputs = inputs.numpy() ### get inputs\n",
    "#                     preds = model(numpy_inputs) ### make prediction from trined model\n",
    "#                     numpy_preds = preds.numpy() ### get predictions\n",
    "\n",
    "#                     all_preds_by_time = []\n",
    "#                     all_labels_by_time = []\n",
    "\n",
    "#                     for j in range(numpy_labels.shape[1]): ## number of time steps\n",
    "#                         ### get values for each bacth and time and de-normalize\n",
    "#                         #print(f'j = {j}')\n",
    "#                         batch_pred =numpy_preds[:,j,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "#                         batch_label = numpy_labels[:,j,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "\n",
    "#                         all_preds_by_time.extend(batch_pred)\n",
    "#                         #print(f'all_preds_by_time = {len(all_preds_by_time_0)}')\n",
    "#                         all_labels_by_time.extend(batch_label)\n",
    "\n",
    "\n",
    "#                     all_preds.append(all_preds_by_time)\n",
    "#                     all_labels.append(all_labels_by_time)\n",
    "#                     if len(all_preds) >= i:\n",
    "#                         break\n",
    "\n",
    "#                     ## covert to array (shape= i,time*batch_size)\n",
    "#             multi_preds = np.vstack(all_preds)\n",
    "#             multi_labels = np.vstack(all_labels)\n",
    "\n",
    "#             mae_pred = []\n",
    "#             r2_pred = []        \n",
    "#             for a in np.arange(0,multi_labels.shape[1],step=32):\n",
    "#                 mae = mean_absolute_error(multi_labels[:,a:a+32], multi_preds[:,a:a+32])\n",
    "#                 mae_pred.append(mae)\n",
    "#                 r2 = round(r2_score(multi_labels[:,a:a+32], multi_preds[:,a:a+32]),3)\n",
    "#                 r2_pred.append(r2)\n",
    "#             df = pd.DataFrame(mae_pred, columns=['mae'])\n",
    "#             df['r2']=r2_pred\n",
    "#             accuracy[plot_col[col]] = {'r2':r2_pred,\n",
    "#                                        'mae':mae_pred}\n",
    "\n",
    "#             if plot:\n",
    "#                 fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#                 plt.suptitle(f'{model}, window: {self.input_width}_{self.shift}',fontsize = 14)\n",
    "#                 ax[0].plot(df.time, df.mae, '-o',c='#ff5555')\n",
    "#                 ax[0].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "#                 ax[0].set_ylabel(f'MAE {plot_col[col]} [de-normed]')\n",
    "#                 ax[1].plot(df.time, df.r2, c='#0ca4b4')\n",
    "#                 ax[1].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "#                 ax[1].set_ylabel(f'R2 {plot_col[col]} [de-normed]')\n",
    "            \n",
    "#     return accuracy\n",
    "                       \n",
    "# WindowGenerator.get_predictions = get_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WindowGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0fb8b78d17f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mWindowGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'WindowGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "def get_predictions(self, model = None, plot_col = ['ATAvg','RHAvg'], plot=False,scaler_type = 'mean'):\n",
    "    \n",
    "    accuracy={}\n",
    "    for col in range(len(plot_col)):\n",
    "        plot_col_index = self.column_indices[plot_col[col]]\n",
    "        all_preds=[]\n",
    "        all_labels =[]\n",
    "    \n",
    "        n_batches = len(tuple(self.test))\n",
    "        if self.shift==1:\n",
    "            for i in range(n_batches):\n",
    "                for inputs, labels in self.test.take(i):  # iterate over batches\n",
    "\n",
    "                    numpy_labels = labels.numpy() ### get labels\n",
    "                    numpy_inputs = inputs.numpy() ### get inputs\n",
    "                    preds = model(numpy_inputs) ### make prediction from trained model\n",
    "                    numpy_preds = preds.numpy() ### get predictions\n",
    "                    \n",
    "                    if scaler_type == 'mean':\n",
    "                        batch_pred =numpy_preds[:,-1,plot_col_index]*test_std[plot_col_index] + test_mean[plot_col_index]\n",
    "                        batch_label = numpy_labels[:,-1,plot_col_index]*test_std[plot_col_index] + test_mean[plot_col_index]\n",
    "                    \n",
    "                    if scaler_type == 'minmax':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        obj = scaler.fit(test_df_raw)\n",
    "                        batch_pred = obj.inverse_transform(numpy_preds[:,-1,:])[:,plot_col_index]\n",
    "                        batch_label = obj.inverse_transform(numpy_labels[:,-1,:])[:,plot_col_index]\n",
    "                        \n",
    "                    if scaler_type == 'stand':\n",
    "                        batch_pred =numpy_preds[:,-1,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "                        batch_label = numpy_labels[:,-1,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "                        \n",
    "\n",
    "                    all_preds.extend(batch_pred)\n",
    "                    all_labels.extend(batch_label)\n",
    "\n",
    "\n",
    "            r2=round(r2_score(all_labels, all_preds),3)\n",
    "            mae = round(mean_absolute_error(all_labels, all_preds),3)\n",
    "\n",
    "            accuracy[plot_col[col]] = {'r2':r2,\n",
    "                                       'mae':mae}\n",
    "\n",
    "            if plot:\n",
    "                ## One sactter plot per variable\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(all_labels, all_preds)\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "                ax.scatter(all_labels, all_preds,edgecolors='k', c='#ff5555', s=32)\n",
    "                ax.set_xlabel(f'True Values {plot_col[col]} [de-normed]')\n",
    "                ax.set_ylabel(f'Predicted Values {plot_col[col]} [de-normed]')\n",
    "                lims = [math.floor(min(all_labels)-min(all_labels)*0.1), math.ceil(max(all_labels)+max(all_labels)*0.1)]\n",
    "                ax.set_xlim(lims)\n",
    "                ax.set_ylim(lims)\n",
    "                line = slope*np.array(all_labels)+intercept\n",
    "                ax.plot(all_labels, line, 'gray',label = f'r2 = {round(r_value**2,3)}')\n",
    "                ax.legend(loc = 'lower right')\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in range(n_batches):\n",
    "                #print(f'i = {i}')\n",
    "                for inputs, labels in self.test.take(i):  # iterate over batches\n",
    "\n",
    "                    numpy_labels = labels.numpy() ### get labels\n",
    "                    numpy_inputs = inputs.numpy() ### get inputs\n",
    "                    preds = model(numpy_inputs) ### make prediction from trined model\n",
    "                    numpy_preds = preds.numpy() ### get predictions\n",
    "\n",
    "                    all_preds_by_time = []\n",
    "                    all_labels_by_time = []\n",
    "\n",
    "                    for j in range(numpy_labels.shape[1]): ## number of time steps\n",
    "                        ### get values for each bacth and time and de-normalize\n",
    "                        #print(f'j = {j}')\n",
    "                        if scaler_type == 'mean':\n",
    "                            batch_pred =numpy_preds[:,j,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "                            batch_label = numpy_labels[:,j,plot_col_index]*train_std[plot_col_index] + train_mean[plot_col_index]\n",
    "                            \n",
    "                        if scaler_type == 'minmax':\n",
    "                            scaler = MinMaxScaler()\n",
    "                            obj = scaler.fit(test_df_raw)\n",
    "                            batch_pred =obj.inverse_transform(numpy_preds[:,j,:])[:,plot_col_index]\n",
    "                            batch_label = obj.inverse_transform(numpy_labels[:,j,:])[:,plot_col_index]\n",
    "                        \n",
    "                        if scaler_type == 'stand':\n",
    "                            scaler = MinMaxScaler()\n",
    "                            obj = scaler.fit(test_df_raw)\n",
    "                            batch_pred =scaler.inverse_transform(numpy_preds[:,j,:])[:,plot_col_index]\n",
    "                            batch_label = scaler.inverse_transform(numpy_labels[:,j,:])[:,plot_col_index]\n",
    "\n",
    "                        all_preds_by_time.extend(batch_pred)\n",
    "                        #print(f'all_preds_by_time = {len(all_preds_by_time_0)}')\n",
    "                        all_labels_by_time.extend(batch_label)\n",
    "\n",
    "\n",
    "                    all_preds.append(all_preds_by_time)\n",
    "                    all_labels.append(all_labels_by_time)\n",
    "                    if len(all_preds) >= i:\n",
    "                        break\n",
    "\n",
    "                    ## covert to array (shape= i,time*batch_size)\n",
    "            multi_preds = np.vstack(all_preds)\n",
    "            multi_labels = np.vstack(all_labels)\n",
    "\n",
    "            mae_pred = []\n",
    "            r2_pred = []\n",
    "            mse_pred =[]\n",
    "            rmse_pred = []\n",
    "            for a in np.arange(0,multi_labels.shape[1],step=batch_size):\n",
    "                mae = mean_absolute_error(multi_labels[:,a:a+batch_size], multi_preds[:,a:a+batch_size])\n",
    "                mae_pred.append(mae)\n",
    "                mse = mean_squared_error(multi_labels[:,a:a+batch_size], multi_preds[:,a:a+batch_size])\n",
    "                mse_pred.append(mse)\n",
    "                rmse = math.sqrt(mse)\n",
    "                rmse_pred.append(rmse)\n",
    "                r2 = round(r2_score(multi_labels[:,a:a+batch_size], multi_preds[:,a:a+batch_size]),3)\n",
    "                r2_pred.append(r2)\n",
    "            df = pd.DataFrame(mae_pred, columns=['mae'])\n",
    "            df['r2']=r2_pred\n",
    "            df['mse']=mse_pred\n",
    "            df['rmse']=rmse_pred\n",
    "            accuracy[plot_col[col]] = {'r2':r2_pred,\n",
    "                                       'mae':mae_pred,\n",
    "                                       'mse': mse_pred,\n",
    "                                       'rmse':rmse_pred}\n",
    "\n",
    "            if plot:\n",
    "                fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                plt.suptitle(f'{model}, window: {self.input_width}_{self.shift}',fontsize = 14)\n",
    "                ax[0].plot(df.index, df.mae, '-o',c='#ff5555')\n",
    "                ax[0].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "                ax[0].set_ylabel(f'MAE {plot_col[col]} [de-normed]')\n",
    "                ax[3].plot(df.index, df.r2,'-o', c='#0ca4b4')\n",
    "                ax[3].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "                ax[3].set_ylabel(f'R2 {plot_col[col]} [de-normed]')\n",
    "                ax[1].plot(df.index, df.mse,'-o', c='#ff5555')\n",
    "                ax[1].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "                ax[1].set_ylabel(f'MSE {plot_col[col]} [de-normed]')\n",
    "                ax[2].plot(df.index, df.rmse, '-o',c='#ff5555')\n",
    "                ax[2].set_xlabel(f'prediction times {plot_col[col]}')\n",
    "                ax[2].set_ylabel(f'RMSE {plot_col[col]} [de-normed]')\n",
    "            \n",
    "    return accuracy\n",
    "                       \n",
    "WindowGenerator.get_predictions = get_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_models(station,path,num_features,input_width, OUT_STEPS):\n",
    "    ### aggragte results\n",
    "    val_performance = {}\n",
    "    performance = {}\n",
    "    r2 ={}\n",
    "    \n",
    "    ## window\n",
    "    window = WindowGenerator(\n",
    "    input_width=input_width, label_width=1, shift=OUT_STEPS)\n",
    "    window.plot()\n",
    "    plt.savefig(f'{path}/{station}_single_{sample_freq}m_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Baseline\n",
    "    baseline = Baseline()\n",
    "    baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    val_performance[f'Baseline_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = baseline.evaluate(window.val)\n",
    "    performance[f'Baseline_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = baseline.evaluate(window.test, verbose=0)\n",
    "    r2[f'Baseline_{sample_freq}m_w{input_width}_{OUT_STEPS}']= window.get_predictions(model=baseline,plot=False)\n",
    "#     window.plot(baseline)\n",
    "#     plt.savefig(f'{path}/{station}_Single_Baseline_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Dense\n",
    "    dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_features)\n",
    "    ])\n",
    "    history = compile_and_fit(dense, window)\n",
    "    IPython.display.clear_output()\n",
    "    val_performance[f'Dense_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = dense.evaluate(window.val)\n",
    "    performance[f'Dense_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = dense.evaluate(window.test, verbose=0)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_Single_Dense_{sample_freq}_w{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(dense)\n",
    "#     plt.savefig(f'{path}/{station}_Single_Dense_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    r2[f'Dense_{sample_freq}m_w{input_width}_{OUT_STEPS}']= window.get_predictions(model=dense,plot=False)\n",
    "    \n",
    "    #RNN\n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=num_features)\n",
    "    ])\n",
    "    history = compile_and_fit(lstm_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    val_performance[f'LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = lstm_model.evaluate(window.val)\n",
    "    performance[f'LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = lstm_model.evaluate(window.test, verbose=0)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_Single_LSTM_{sample_freq}_w{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(lstm_model)\n",
    "#     plt.savefig(f'{path}/{station}_Single_LSTM_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    r2[f'LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}']= window.get_predictions(model=lstm_model,plot=False)\n",
    "    \n",
    "    ### Autoregressive RNN\n",
    "    residual_lstm = ResidualWrapper(\n",
    "    tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        num_features,\n",
    "        # The predicted deltas should start small\n",
    "        # So initialize the output layer with zeros\n",
    "        kernel_initializer=tf.initializers.zeros())\n",
    "    ]))\n",
    "\n",
    "    history = compile_and_fit(residual_lstm, window)\n",
    "\n",
    "    IPython.display.clear_output()\n",
    "    val_performance[f'Residual_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = residual_lstm.evaluate(window.val)\n",
    "    performance[f'Residual_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = residual_lstm.evaluate(window.test, verbose=0)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_Single_Residual_LSTM_{sample_freq}_w{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(lstm_model)\n",
    "#     plt.savefig(f'{path}/{station}_Single_Residual_LSTM_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    r2[f'Residual_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}']= window.get_predictions(model=residual_lstm,plot=False)\n",
    "    \n",
    "\n",
    "    ### Performance\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(len(performance))\n",
    "    width = 0.3\n",
    "    metric_name = 'mean_absolute_error'\n",
    "    metric_index = baseline.metrics_names.index('mean_absolute_error')\n",
    "    val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "    test_mae = [v[metric_index] for v in performance.values()]\n",
    "\n",
    "    ax.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "    ax.bar(x + 0.17, test_mae, width, label='Test')\n",
    "    ax.set_xticks(ticks=x)\n",
    "           \n",
    "    ax.set_xticklabels(labels=performance.keys(),rotation = 45, ha=\"right\")\n",
    "    ax.set_ylabel('MAE (average over all outputs)')\n",
    "    ax.legend()\n",
    "    plt.savefig(f'{path}/{station}_single_{sample_freq}m_wm{input_width}_{OUT_STEPS}_performance.png', dpi = 100,bbox_inches='tight')\n",
    "    \n",
    "    pd.concat({k: pd.DataFrame(v).T for k, v in r2.items()}, axis=0).to_csv(f'{path}/{station}_single_{sample_freq}m_w{input_width}_{OUT_STEPS}_performance_times.csv')\n",
    "    per = pd.DataFrame.from_dict(multi_performance, orient='index',columns=['loss_test','mae_test'])\n",
    "    val= pd.DataFrame.from_dict(multi_val_performance, orient='index',columns=['loss_val','mae_val'])\n",
    "    pd.merge(per, val, how='inner',left_index=True, right_index =True).to_csv(f'{path}/{station}_single_{sample_freq}m_w{input_width}_{OUT_STEPS}_performance_overall.csv')\n",
    "    \n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_models(station, path, num_features,input_width= 24, OUT_STEPS=12):\n",
    "    ### aggragte results\n",
    "    multi_val_performance = {}\n",
    "    multi_performance = {}\n",
    "    r2 ={}\n",
    "    \n",
    "    ## window\n",
    "    window = WindowGenerator(\n",
    "    input_width=input_width, label_width=OUT_STEPS, shift=OUT_STEPS)\n",
    "    window.plot(plot_col=list(window.column_indices.keys())[0])\n",
    "    plt.savefig(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Baseline last\n",
    "    print('Baseline last')\n",
    "    last_baseline = MultiStepLastBaseline()\n",
    "    last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    multi_val_performance[f'BaselineLast_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = last_baseline.evaluate(window.val)\n",
    "    multi_performance[f'BaselineLast_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = last_baseline.evaluate(window.test, verbose=0)\n",
    "    r2[f'BaselineLast_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=last_baseline, plot_col =vars_to_analize)\n",
    "#     window.plot(last_baseline)\n",
    "#     plt.savefig(f'{path}/{station}_BaselineLast_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Baseline repeat\n",
    "    if input_width == OUT_STEPS:\n",
    "        print(f'Baseline repeat')\n",
    "        repeat_baseline = RepeatBaseline()\n",
    "        repeat_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        multi_val_performance[f'BaselineRepeat_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = repeat_baseline.evaluate(window.val)\n",
    "        multi_performance[f'BaselineRepeat_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = repeat_baseline.evaluate(window.test, verbose=0)\n",
    "        r2[f'BaselineRepeat_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=repeat_baseline,plot_col =vars_to_analize)\n",
    "#         window.plot(repeat_baseline)\n",
    "#         plt.savefig(f'{path}/{station}_BaselineRepeat_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    else:\n",
    "        print('Skipping Repeat baseline')\n",
    "    \n",
    "    ### Single-Shot\n",
    "    print(f'Single-shot')\n",
    "    multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "    ])\n",
    "\n",
    "    history = compile_and_fit(multi_linear_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    multi_val_performance[f'MultiLinear_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_linear_model.evaluate(window.val)\n",
    "    multi_performance[f'MultiLinear_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_linear_model.evaluate(window.test, verbose=0)\n",
    "    r2[f'MultiLinear_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=multi_linear_model,plot_col =vars_to_analize)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_MultiLinear_wm{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(multi_linear_model)\n",
    "#     plt.savefig(f'{path}/{station}_MultiLinear_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Dense\n",
    "    print(f'Dense')\n",
    "    multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "    ])\n",
    "\n",
    "    history = compile_and_fit(multi_dense_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    multi_val_performance[f'MultiDense_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_dense_model.evaluate(window.val)\n",
    "    multi_performance[f'MultiDense_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_dense_model.evaluate(window.test, verbose=0)\n",
    "    r2[f'MultiDense_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=multi_dense_model,plot_col =vars_to_analize)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_MultiDense_wm{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(multi_dense_model)\n",
    "#     plt.savefig(f'{path}/{station}_MultiDense_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### CNN\n",
    "    print(f'CNN')\n",
    "    CONV_WIDTH = 3\n",
    "    multi_conv_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "    # Shape => [batch, 1, conv_units]\n",
    "    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "    ])\n",
    "\n",
    "    history = compile_and_fit(multi_conv_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    multi_val_performance[f'MultiConv_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_conv_model.evaluate(window.val)\n",
    "    multi_performance[f'MultiConv_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_conv_model.evaluate(window.test, verbose=0)\n",
    "    r2[f'MultiConv_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=multi_conv_model,plot_col =vars_to_analize)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_MultiConv_wm{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(multi_conv_model)\n",
    "#     plt.savefig(f'{path}/{station}_MultiConv_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### RNN\n",
    "    print(f'RNN')\n",
    "    multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "    ])\n",
    "\n",
    "    history = compile_and_fit(multi_lstm_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    multi_val_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_lstm_model.evaluate(window.val)\n",
    "    multi_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = multi_lstm_model.evaluate(window.test, verbose=0)\n",
    "    r2[f'MultiLSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=multi_lstm_model,plot_col =vars_to_analize)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_MultiLSTM_wm{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(multi_lstm_model)\n",
    "#     plt.savefig(f'{path}/{station}_MultiLSTM_{sample_freq}_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "    \n",
    "    ### Autoregressive RNN\n",
    "    print(f'Autoregressive RNN')\n",
    "    feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)\n",
    "    prediction, state = feedback_model.warmup(window.example[0])\n",
    "    history = compile_and_fit(feedback_model, window)\n",
    "    IPython.display.clear_output()\n",
    "    multi_val_performance[f'AR_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = feedback_model.evaluate(window.val)\n",
    "    multi_performance[f'AR_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = feedback_model.evaluate(window.test, verbose=0)\n",
    "    r2[f'AR_LSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}'] = window.get_predictions(model=multi_lstm_model,plot_col =vars_to_analize)\n",
    "#     losses = pd.DataFrame(history.history)\n",
    "#     losses.plot()\n",
    "#     plt.savefig(f'{path}/{station}_MultiAR_LSTM_{sample_freq}_wm{input_width}_{OUT_STEPS}_losses.png',dpi=100)\n",
    "#     window.plot(feedback_model)\n",
    "#     plt.savefig(f'{path}/{station}_MultiAR_LSTM_wm{input_width}_{OUT_STEPS}_window.png',dpi=100)\n",
    "\n",
    "    ### Plot Performance\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(len(multi_performance))\n",
    "    width = 0.3\n",
    "    metric_name = 'mean_absolute_error'\n",
    "    metric_index = last_baseline.metrics_names.index('mean_absolute_error')\n",
    "    val_mae = [v[metric_index] for v in multi_val_performance.values()]\n",
    "    test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "    ax.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "    ax.bar(x + 0.17, test_mae, width, label='Test')\n",
    "    ax.set_xticks(ticks=x)\n",
    "           \n",
    "    ax.set_xticklabels(labels=multi_performance.keys(),rotation = 45, ha=\"right\")\n",
    "    ax.set_ylabel('MAE (average over all outputs)')\n",
    "    ax.legend()\n",
    "    plt.savefig(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_performance.png', dpi = 100,bbox_inches='tight')\n",
    "    \n",
    "    pd.concat({k: pd.DataFrame(v).T for k, v in r2.items()}, axis=0).to_csv(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_performance_times.csv')\n",
    "    per = pd.DataFrame.from_dict(multi_performance, orient='index',columns=['loss_test','mae_test'])\n",
    "    val= pd.DataFrame.from_dict(multi_val_performance, orient='index',columns=['loss_val','mae_val'])\n",
    "    pd.merge(per, val, how='inner',left_index=True, right_index =True).to_csv(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_performance_overall.csv')\n",
    "    \n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_times(file,y0=0.5,y1=1.5, nplots = 13, nrows=5, ncols = 3):\n",
    "    times = pd.read_csv(file)\n",
    "    models = times['Unnamed: 0'].unique()\n",
    "    variables = times['Unnamed: 1'].unique()\n",
    "\n",
    "    colors = ['#c8ea53','#f56420','#7167ce','#15c534','#e9dc09','#38a9f0','#702dae']\n",
    "    model_color = dict(zip(models,colors))\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows,ncols,figsize= (15,15))\n",
    "    if nrows*ncols - nplots ==1:\n",
    "        fig.delaxes(axes[4,2]) \n",
    "    if nrows*ncols - nplots ==2:\n",
    "        fig.delaxes(axes[4,1])\n",
    "        fig.delaxes(axes[4,2]) \n",
    "    \n",
    "    for a, ax in enumerate(axes.flatten()):\n",
    "        timet =times[times['Unnamed: 1'] == variables[a]]\n",
    "        if len(models)==1:\n",
    "            model_start = 0\n",
    "        else: model_start = 1\n",
    "        for model in models[model_start:]:\n",
    "                val_index =timet.index[timet['Unnamed: 0'] ==model].values[0]\n",
    "                y = json.loads(timet[timet['Unnamed: 0'] ==model].mae[val_index])\n",
    "                x = np.arange(1,len(y)+1)\n",
    "                ax.plot(x,y,'-o',color = model_color[model],label=model.split('_')[0])\n",
    "                ax.set_title(variables[a])\n",
    "                ax.set_ylim(y0,y1)\n",
    "                if a==0:\n",
    "                    ax.legend()\n",
    "    return plt.savefig(f'{file.split(\".\")[0]}.png', dpi = 100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
