{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One station LSTM\n",
    "LSTM_tensorflow_tutorial [link](https://www.tensorflow.org/tutorials/structured_data/time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import math\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_ids = {'guadalupe':'guadalupe_validation.csv',\n",
    "         'banos':'banos_validation.csv',\n",
    "         'aeropuerto': 'aeropuerto_validation.csv',\n",
    "         'quisapincha':'quisapincha_validation.csv',\n",
    "         'chiquiurco':'chiquiurco_validation.csv',\n",
    "         'AJSucre':'AJSucre_validation.csv',\n",
    "         'JAlvarez':'JAlvarez_validation.csv',\n",
    "         'pfc-hgpt':'pfc-hgpt_validation.csv',\n",
    "         'calamaca':'calamaca_validation.csv',\n",
    "         'mulaCorral':'mulaCorral_validation.csv',\n",
    "         'pampasSalasaca':'pampasSalasaca_validation.csv',\n",
    "         'tasinteo':'tasinteo_validation.csv',\n",
    "         'pisayambo':'pisayambo_validation.csv'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set fixed variables\n",
    "input_path = 'data/PROCESSED/STATIONS_CLEAN'\n",
    "path = 'data/PROCESSED/MODEL_OUTPUTS'\n",
    "station = 'guadalupe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling frequency\n",
    "sample_freq = 60 #(time in minutes)\n",
    "steps=int(sample_freq/5)\n",
    "time_steps = int(60/sample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{input_path}/{station}_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Subsample to get data for every hour (starting from index 0, get 12 steps)\n",
    "df = df[0::steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = pd.to_datetime(df.pop('Date_Time'), format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove PAvg and WMaxx, WMiny, WDAvg and WSAvg\n",
    "df = df[['ATAvg','ATMax','ATMin',\n",
    "         'RHAvg','RHMin','RHMax',\n",
    "         'WAvgx','WAvgy', 'WMinx', 'WMiny',\n",
    "         'Day_sin', 'Day_cos', 'Year_sin', 'Year_cos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into 70%, 20%, 10% split for the training, validation, and test sets\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize the data  ### ROOM TO MAKE TESTS (this is just an average)\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1]\n",
    "vars_to_analize = ['ATAvg','RHAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 48*time_steps\n",
    "OUT_STEPS =24*time_steps\n",
    "#performance = multi_models(station, path, num_features,input_width, OUT_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "r2 ={}\n",
    "\n",
    "## window\n",
    "window = WindowGenerator(\n",
    "input_width=input_width, label_width=OUT_STEPS, shift=OUT_STEPS)\n",
    "window.plot(plot_col=list(window.column_indices.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN\n",
    "print(f'RNN')\n",
    "\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history=compile_and_fit(multi_lstm_model, window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.val)\n",
    "multi_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.test, verbose=0)\n",
    "r2[f'MultiLSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = window.get_predictions(model=multi_lstm_model,plot_col =vars_to_analize)\n",
    "\n",
    "losses = pd.DataFrame(history.history)\n",
    "losses.plot()\n",
    "plt.savefig(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_losses.png',dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({k: pd.DataFrame(v).T for k, v in r2.items()}, axis=0).to_csv(f'{path}/{station}_lstm2-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_times.csv')\n",
    "per = pd.DataFrame.from_dict(multi_performance, orient='index',columns=['loss_test','mae_test'])\n",
    "val= pd.DataFrame.from_dict(multi_val_performance, orient='index',columns=['loss_val','mae_val'])\n",
    "pd.merge(per, val, how='inner',left_index=True, right_index =True).to_csv(f'{path}/{station}_lstm2-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_overall.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN\n",
    "print(f'RNN')\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "r2 ={}\n",
    "\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "compile_and_fit(multi_lstm_model, window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.val)\n",
    "multi_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.test, verbose=0)\n",
    "r2[f'MultiLSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = window.get_predictions(model=multi_lstm_model,plot_col =vars_to_analize)\n",
    "\n",
    "history = losses = pd.DataFrame(history.history)\n",
    "losses.plot()\n",
    "plt.savefig(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_losses.png',dpi=100)\n",
    "\n",
    "pd.concat({k: pd.DataFrame(v).T for k, v in r2.items()}, axis=0).to_csv(f'{path}/{station}_lstm3-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_times.csv')\n",
    "per = pd.DataFrame.from_dict(multi_performance, orient='index',columns=['loss_test','mae_test'])\n",
    "val= pd.DataFrame.from_dict(multi_val_performance, orient='index',columns=['loss_val','mae_val'])\n",
    "pd.merge(per, val, how='inner',left_index=True, right_index =True).to_csv(f'{path}/{station}_lstm3-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_overall.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN\n",
    "print(f'RNN')\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "r2 ={}\n",
    "\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.LSTM(batch_size, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history =compile_and_fit(multi_lstm_model, window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.val)\n",
    "multi_performance[f'MultiLSTM_model_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = multi_lstm_model.evaluate(window.test, verbose=0)\n",
    "r2[f'MultiLSTM_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}'] = window.get_predictions(model=multi_lstm_model,plot_col =vars_to_analize)\n",
    "\n",
    "losses = pd.DataFrame(history.history)\n",
    "losses.plot()\n",
    "plt.savefig(f'{path}/{station}_multi_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_losses.png',dpi=100)\n",
    "\n",
    "pd.concat({k: pd.DataFrame(v).T for k, v in r2.items()}, axis=0).to_csv(f'{path}/{station}_lstm4-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_times.csv')\n",
    "per = pd.DataFrame.from_dict(multi_performance, orient='index',columns=['loss_test','mae_test'])\n",
    "val= pd.DataFrame.from_dict(multi_val_performance, orient='index',columns=['loss_val','mae_val'])\n",
    "pd.merge(per, val, how='inner',left_index=True, right_index =True).to_csv(f'{path}/{station}_lstm4-{batch_size}_{sample_freq}m_w{input_width}_{OUT_STEPS}_{MAX_EPOCHS}_performance_overall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
